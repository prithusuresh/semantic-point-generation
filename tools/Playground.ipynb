{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "544164df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep imports here\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from pcdet.ops.voxel import Voxelization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23938f97",
   "metadata": {},
   "source": [
    "##Load data and voxelize\n",
    "# kitti config file\n",
    "DATASET: 'KittiDataset'\n",
    "DATA_PATH: '../data/kitti'\n",
    "\n",
    "POINT_CLOUD_RANGE: [0, -40, -3, 70.4, 40, 1]\n",
    "\n",
    "DATA_SPLIT: {\n",
    "    'train': train,\n",
    "    'test': val\n",
    "}\n",
    "\n",
    "INFO_PATH: {\n",
    "    'train': [kitti_infos_train.pkl],\n",
    "    'test': [kitti_infos_val.pkl],\n",
    "}\n",
    "\n",
    "GET_ITEM_LIST: [\"points\"]\n",
    "FOV_POINTS_ONLY: True\n",
    "\n",
    "DATA_AUGMENTOR:\n",
    "    DISABLE_AUG_LIST: ['placeholder']\n",
    "    AUG_CONFIG_LIST:\n",
    "        - NAME: gt_sampling\n",
    "          USE_ROAD_PLANE: True\n",
    "          DB_INFO_PATH:\n",
    "              - kitti_dbinfos_train.pkl\n",
    "          PREPARE: {\n",
    "             filter_by_min_points: ['Car:5', 'Pedestrian:5', 'Cyclist:5'],\n",
    "             filter_by_difficulty: [-1],\n",
    "          }\n",
    "\n",
    "          SAMPLE_GROUPS: ['Car:20','Pedestrian:15', 'Cyclist:15']\n",
    "          NUM_POINT_FEATURES: 4\n",
    "          DATABASE_WITH_FAKELIDAR: False\n",
    "          REMOVE_EXTRA_WIDTH: [0.0, 0.0, 0.0]\n",
    "          LIMIT_WHOLE_SCENE: True\n",
    "\n",
    "        - NAME: random_world_flip\n",
    "          ALONG_AXIS_LIST: ['x']\n",
    "\n",
    "        - NAME: random_world_rotation\n",
    "          WORLD_ROT_ANGLE: [-0.78539816, 0.78539816]\n",
    "\n",
    "        - NAME: random_world_scaling\n",
    "          WORLD_SCALE_RANGE: [0.95, 1.05]\n",
    "\n",
    "\n",
    "POINT_FEATURE_ENCODING: {\n",
    "    encoding_type: absolute_coordinates_encoding,\n",
    "    used_feature_list: ['x', 'y', 'z', 'intensity'],\n",
    "    src_feature_list: ['x', 'y', 'z', 'intensity'],\n",
    "}\n",
    "\n",
    "\n",
    "DATA_PROCESSOR:\n",
    "    - NAME: mask_points_and_boxes_outside_range\n",
    "      REMOVE_OUTSIDE_BOXES: True\n",
    "\n",
    "    - NAME: shuffle_points\n",
    "      SHUFFLE_ENABLED: {\n",
    "        'train': True,\n",
    "        'test': False\n",
    "      }\n",
    "\n",
    "    - NAME: transform_points_to_voxels\n",
    "      VOXEL_SIZE: [0.05, 0.05, 0.1]\n",
    "      MAX_POINTS_PER_VOXEL: 5\n",
    "      MAX_NUMBER_OF_VOXELS: {\n",
    "        'train': 16000,\n",
    "        'test': 40000\n",
    "      }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0e5b87d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1f8882d7f59f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mKittiDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKittiDataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "class KittiDataset(Dataset):\n",
    "    def __init__(self, cfg):\n",
    "        super(KittiDataset,self).__init__()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem(self,x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98144add",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86afc054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcdet.models.detectors.pointpillar import PointPillar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed739bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['frame_id', 'gt_boxes', 'points', 'use_lead_xyz', 'voxels', 'voxel_coords', 'voxel_num_points', 'image_shape', 'batch_size'])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pcdet.config import cfg, cfg_from_yaml_file\n",
    "from eval_utils.eval_utils import load_data_to_gpu\n",
    "import pickle\n",
    "cfg = cfg_from_yaml_file(\"cfgs/kitti_models/spg.yaml\", cfg)\n",
    "model_cfg = cfg.MODEL\n",
    "\n",
    "train_sample = pickle.load(open(\"train_sample.p\", \"rb\"))\n",
    "print (train_sample.keys())\n",
    "train_sample[\"gt_classification\"] = np.zeros(train_sample[\"batch_size\"], train_sample[\"voxel_num_points\"][0])\n",
    "print (train_sample[\"batch_size\"])\n",
    "load_data_to_gpu(train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91e07884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16, 0.16, 0.1]\n"
     ]
    }
   ],
   "source": [
    "for p in cfg.DATA_CONFIG.DATA_PROCESSOR:\n",
    "    if p.NAME == \"transform_points_to_smaller_voxels\":\n",
    "        small_voxel_size = p.VOXEL_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e805913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'module_list': [],\n",
       " 'num_rawpoint_features': 4,\n",
       " 'num_point_features': 4,\n",
       " 'grid_size': array([432, 496,   1]),\n",
       " 'point_cloud_range': array([  0.  , -39.68,  -3.  ,  69.12,  39.68,   1.  ]),\n",
       " 'voxel_size': [0.16, 0.16, 4],\n",
       " 'z_voxel_size': 0.1,\n",
       " 'depth_downsample_factor': None,\n",
       " 'out_classes': 40}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SPG MODEL\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "model_info_dict = {'module_list': [], \n",
    "                   'num_rawpoint_features': 4, \n",
    "                   'num_point_features': 4, \n",
    "                   'grid_size': np.array([432, 496, 1]), \n",
    "                   'point_cloud_range': np.array([  0.  , -39.68,  -3.  ,  69.12,  39.68,   1.  ]), \n",
    "                   'voxel_size': [0.16, 0.16, 4], \n",
    "                   'z_voxel_size': 0.1,\n",
    "                   'depth_downsample_factor': None}\n",
    "model_info_dict[\"out_classes\"] = int(abs(cfg.DATA_CONFIG.POINT_CLOUD_RANGE[2] - cfg.DATA_CONFIG.POINT_CLOUD_RANGE[5])/model_info_dict[\"z_voxel_size\"])\n",
    "model_info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "becc4454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SPG_CLASSIFICATION(\n",
       "  (vfe_module): PillarVFE(\n",
       "    (pfn_layers): ModuleList(\n",
       "      (0): PFNLayer(\n",
       "        (linear): Linear(in_features=10, out_features=64, bias=False)\n",
       "        (norm): BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (map_to_bev_module): PointPillarScatter()\n",
       "  (backbone_2d_module): BaseBEVBackbone(\n",
       "    (blocks): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (2): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (3): ReLU()\n",
       "        (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (5): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (6): ReLU()\n",
       "        (7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (8): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (9): ReLU()\n",
       "        (10): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (11): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (12): ReLU()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        (1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (2): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (3): ReLU()\n",
       "        (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (5): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (6): ReLU()\n",
       "        (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (8): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (9): ReLU()\n",
       "        (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (11): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (12): ReLU()\n",
       "        (13): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (14): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (15): ReLU()\n",
       "        (16): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (17): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (18): ReLU()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (2): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (3): ReLU()\n",
       "        (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (5): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (6): ReLU()\n",
       "        (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (8): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (9): ReLU()\n",
       "        (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (11): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (12): ReLU()\n",
       "        (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (14): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (15): ReLU()\n",
       "        (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (17): BatchNorm2d(256, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (18): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (deblocks): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): ConvTranspose2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(4, 4), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classification_head): ConvClassificationHead(\n",
       "    (conv): ConvTranspose2d(384, 40, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (act): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VFETemplate(nn.Module):\n",
    "    def __init__(self, model_cfg, **kwargs):\n",
    "        super().__init__()\n",
    "        self.model_cfg = model_cfg\n",
    "\n",
    "    def get_output_feature_dim(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            **kwargs:\n",
    "\n",
    "        Returns:\n",
    "            batch_dict:\n",
    "                ...\n",
    "                vfe_features: (num_voxels, C)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class PFNLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 use_norm=True,\n",
    "                 last_layer=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.last_vfe = last_layer\n",
    "        self.use_norm = use_norm\n",
    "        if not self.last_vfe:\n",
    "            out_channels = out_channels // 2\n",
    "\n",
    "        if self.use_norm:\n",
    "            self.linear = nn.Linear(in_channels, out_channels, bias=False)\n",
    "            self.norm = nn.BatchNorm1d(out_channels, eps=1e-3, momentum=0.01)\n",
    "        else:\n",
    "            self.linear = nn.Linear(in_channels, out_channels, bias=True)\n",
    "\n",
    "        self.part = 50000\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if inputs.shape[0] > self.part:\n",
    "            # nn.Linear performs randomly when batch size is too large\n",
    "            num_parts = inputs.shape[0] // self.part\n",
    "            part_linear_out = [self.linear(inputs[num_part*self.part:(num_part+1)*self.part])\n",
    "                               for num_part in range(num_parts+1)]\n",
    "            x = torch.cat(part_linear_out, dim=0)\n",
    "        else:\n",
    "            x = self.linear(inputs)\n",
    "        torch.backends.cudnn.enabled = False\n",
    "        x = self.norm(x.permute(0, 2, 1)).permute(0, 2, 1) if self.use_norm else x\n",
    "        torch.backends.cudnn.enabled = True\n",
    "        x = F.relu(x)\n",
    "        x_max = torch.max(x, dim=1, keepdim=True)[0]\n",
    "\n",
    "        if self.last_vfe:\n",
    "            return x_max\n",
    "        else:\n",
    "            x_repeat = x_max.repeat(1, inputs.shape[1], 1)\n",
    "            x_concatenated = torch.cat([x, x_repeat], dim=2)\n",
    "            return x_concatenated\n",
    "\n",
    "\n",
    "class PillarVFE(VFETemplate):\n",
    "    def __init__(self, model_cfg, num_point_features, voxel_size, point_cloud_range, **kwargs):\n",
    "        super().__init__(model_cfg=model_cfg)\n",
    "\n",
    "        self.use_norm = self.model_cfg.USE_NORM\n",
    "        self.with_distance = self.model_cfg.WITH_DISTANCE\n",
    "        self.use_absolute_xyz = self.model_cfg.USE_ABSLOTE_XYZ\n",
    "        num_point_features += 6 if self.use_absolute_xyz else 3\n",
    "        if self.with_distance:\n",
    "            num_point_features += 1\n",
    "\n",
    "        self.num_filters = self.model_cfg.NUM_FILTERS\n",
    "        assert len(self.num_filters) > 0\n",
    "        num_filters = [num_point_features] + list(self.num_filters)\n",
    "\n",
    "        pfn_layers = []\n",
    "        for i in range(len(num_filters) - 1):\n",
    "            in_filters = num_filters[i]\n",
    "            out_filters = num_filters[i + 1]\n",
    "            pfn_layers.append(\n",
    "                PFNLayer(in_filters, out_filters, self.use_norm, last_layer=(i >= len(num_filters) - 2))\n",
    "            )\n",
    "        self.pfn_layers = nn.ModuleList(pfn_layers)\n",
    "\n",
    "        self.voxel_x = voxel_size[0]\n",
    "        self.voxel_y = voxel_size[1]\n",
    "        self.voxel_z = voxel_size[2]\n",
    "        self.x_offset = self.voxel_x / 2 + point_cloud_range[0]\n",
    "        self.y_offset = self.voxel_y / 2 + point_cloud_range[1]\n",
    "        self.z_offset = self.voxel_z / 2 + point_cloud_range[2]\n",
    "\n",
    "    def get_output_feature_dim(self):\n",
    "        return self.num_filters[-1]\n",
    "\n",
    "    def get_paddings_indicator(self, actual_num, max_num, axis=0):\n",
    "        actual_num = torch.unsqueeze(actual_num, axis + 1)\n",
    "        max_num_shape = [1] * len(actual_num.shape)\n",
    "        max_num_shape[axis + 1] = -1\n",
    "        max_num = torch.arange(max_num, dtype=torch.int, device=actual_num.device).view(max_num_shape)\n",
    "        paddings_indicator = actual_num.int() > max_num\n",
    "        return paddings_indicator\n",
    "\n",
    "    def forward(self, batch_dict, **kwargs):\n",
    "        voxel_features, voxel_num_points, coords = batch_dict['voxels'], batch_dict['voxel_num_points'], batch_dict['voxel_coords']\n",
    "        points_mean = voxel_features[:, :, :3].sum(dim=1, keepdim=True) / voxel_num_points.type_as(voxel_features).view(-1, 1, 1)\n",
    "        f_cluster = voxel_features[:, :, :3] - points_mean\n",
    "\n",
    "        f_center = torch.zeros_like(voxel_features[:, :, :3])\n",
    "        f_center[:, :, 0] = voxel_features[:, :, 0] - (coords[:, 3].to(voxel_features.dtype).unsqueeze(1) * self.voxel_x + self.x_offset)\n",
    "        f_center[:, :, 1] = voxel_features[:, :, 1] - (coords[:, 2].to(voxel_features.dtype).unsqueeze(1) * self.voxel_y + self.y_offset)\n",
    "        f_center[:, :, 2] = voxel_features[:, :, 2] - (coords[:, 1].to(voxel_features.dtype).unsqueeze(1) * self.voxel_z + self.z_offset)\n",
    "\n",
    "        if self.use_absolute_xyz:\n",
    "            features = [voxel_features, f_cluster, f_center]\n",
    "        else:\n",
    "            features = [voxel_features[..., 3:], f_cluster, f_center]\n",
    "\n",
    "        if self.with_distance:\n",
    "            points_dist = torch.norm(voxel_features[:, :, :3], 2, 2, keepdim=True)\n",
    "            features.append(points_dist)\n",
    "        features = torch.cat(features, dim=-1)\n",
    "\n",
    "        voxel_count = features.shape[1]\n",
    "        mask = self.get_paddings_indicator(voxel_num_points, voxel_count, axis=0)\n",
    "        mask = torch.unsqueeze(mask, -1).type_as(voxel_features)\n",
    "        features *= mask\n",
    "        for pfn in self.pfn_layers:\n",
    "            features = pfn(features)\n",
    "        features = features.squeeze()\n",
    "        batch_dict['pillar_features'] = features\n",
    "        return batch_dict\n",
    "    \n",
    "class PointPillarScatter(nn.Module):\n",
    "    def __init__(self, model_cfg, grid_size, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_cfg = model_cfg\n",
    "        self.num_bev_features = self.model_cfg.NUM_BEV_FEATURES\n",
    "        self.nx, self.ny, self.nz = grid_size\n",
    "        assert self.nz == 1\n",
    "\n",
    "    def forward(self, batch_dict, **kwargs):\n",
    "        pillar_features, coords = batch_dict['pillar_features'], batch_dict['voxel_coords']\n",
    "        batch_spatial_features = []\n",
    "        batch_size = coords[:, 0].max().int().item() + 1\n",
    "        for batch_idx in range(batch_size):\n",
    "            spatial_feature = torch.zeros(\n",
    "                self.num_bev_features,\n",
    "                self.nz * self.nx * self.ny,\n",
    "                dtype=pillar_features.dtype,\n",
    "                device=pillar_features.device)\n",
    "\n",
    "            batch_mask = coords[:, 0] == batch_idx\n",
    "            this_coords = coords[batch_mask, :]\n",
    "            indices = this_coords[:, 1] + this_coords[:, 2] * self.nx + this_coords[:, 3]\n",
    "            indices = indices.type(torch.long)\n",
    "            pillars = pillar_features[batch_mask, :]\n",
    "            pillars = pillars.t()\n",
    "            spatial_feature[:, indices] = pillars\n",
    "            batch_spatial_features.append(spatial_feature)\n",
    "\n",
    "        batch_spatial_features = torch.stack(batch_spatial_features, 0)\n",
    "        batch_spatial_features = batch_spatial_features.view(batch_size, self.num_bev_features * self.nz, self.ny, self.nx)\n",
    "        batch_dict['spatial_features'] = batch_spatial_features\n",
    "        return batch_dict\n",
    "\n",
    "class BaseBEVBackbone(nn.Module):\n",
    "    def __init__(self, model_cfg, input_channels):\n",
    "        super().__init__()\n",
    "        self.model_cfg = model_cfg\n",
    "\n",
    "        if self.model_cfg.get('LAYER_NUMS', None) is not None:\n",
    "            assert len(self.model_cfg.LAYER_NUMS) == len(self.model_cfg.LAYER_STRIDES) == len(self.model_cfg.NUM_FILTERS)\n",
    "            layer_nums = self.model_cfg.LAYER_NUMS\n",
    "            layer_strides = self.model_cfg.LAYER_STRIDES\n",
    "            num_filters = self.model_cfg.NUM_FILTERS\n",
    "        else:\n",
    "            layer_nums = layer_strides = num_filters = []\n",
    "\n",
    "        if self.model_cfg.get('UPSAMPLE_STRIDES', None) is not None:\n",
    "            assert len(self.model_cfg.UPSAMPLE_STRIDES) == len(self.model_cfg.NUM_UPSAMPLE_FILTERS)\n",
    "            num_upsample_filters = self.model_cfg.NUM_UPSAMPLE_FILTERS\n",
    "            upsample_strides = self.model_cfg.UPSAMPLE_STRIDES\n",
    "        else:\n",
    "            upsample_strides = num_upsample_filters = []\n",
    "\n",
    "        num_levels = len(layer_nums)\n",
    "        c_in_list = [input_channels, *num_filters[:-1]]\n",
    "        self.blocks = nn.ModuleList()\n",
    "        self.deblocks = nn.ModuleList()\n",
    "        for idx in range(num_levels):\n",
    "            cur_layers = [\n",
    "                nn.ZeroPad2d(1),\n",
    "                nn.Conv2d(\n",
    "                    c_in_list[idx], num_filters[idx], kernel_size=3,\n",
    "                    stride=layer_strides[idx], padding=0, bias=False\n",
    "                ),\n",
    "                nn.BatchNorm2d(num_filters[idx], eps=1e-3, momentum=0.01),\n",
    "                nn.ReLU()\n",
    "            ]\n",
    "            for k in range(layer_nums[idx]):\n",
    "                cur_layers.extend([\n",
    "                    nn.Conv2d(num_filters[idx], num_filters[idx], kernel_size=3, padding=1, bias=False),\n",
    "                    nn.BatchNorm2d(num_filters[idx], eps=1e-3, momentum=0.01),\n",
    "                    nn.ReLU()\n",
    "                ])\n",
    "            self.blocks.append(nn.Sequential(*cur_layers))\n",
    "            if len(upsample_strides) > 0:\n",
    "                stride = upsample_strides[idx]\n",
    "                if stride >= 1:\n",
    "                    self.deblocks.append(nn.Sequential(\n",
    "                        nn.ConvTranspose2d(\n",
    "                            num_filters[idx], num_upsample_filters[idx],\n",
    "                            upsample_strides[idx],\n",
    "                            stride=upsample_strides[idx], bias=False\n",
    "                        ),\n",
    "                        nn.BatchNorm2d(num_upsample_filters[idx], eps=1e-3, momentum=0.01),\n",
    "                        nn.ReLU()\n",
    "                    ))\n",
    "                else:\n",
    "                    stride = np.round(1 / stride).astype(np.int)\n",
    "                    self.deblocks.append(nn.Sequential(\n",
    "                        nn.Conv2d(\n",
    "                            num_filters[idx], num_upsample_filters[idx],\n",
    "                            stride,\n",
    "                            stride=stride, bias=False\n",
    "                        ),\n",
    "                        nn.BatchNorm2d(num_upsample_filters[idx], eps=1e-3, momentum=0.01),\n",
    "                        nn.ReLU()\n",
    "                    ))\n",
    "\n",
    "        c_in = sum(num_upsample_filters)\n",
    "        if len(upsample_strides) > num_levels:\n",
    "            self.deblocks.append(nn.Sequential(\n",
    "                nn.ConvTranspose2d(c_in, c_in, upsample_strides[-1], stride=upsample_strides[-1], bias=False),\n",
    "                nn.BatchNorm2d(c_in, eps=1e-3, momentum=0.01),\n",
    "                nn.ReLU(),\n",
    "            ))\n",
    "\n",
    "        self.num_bev_features = c_in\n",
    "\n",
    "    def forward(self, data_dict):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dict:\n",
    "                spatial_features\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        spatial_features = data_dict['spatial_features']\n",
    "        ups = []\n",
    "        ret_dict = {}\n",
    "        x = spatial_features\n",
    "        for i in range(len(self.blocks)):\n",
    "            x = self.blocks[i](x)\n",
    "\n",
    "            stride = int(spatial_features.shape[2] / x.shape[2])\n",
    "            ret_dict['spatial_features_%dx' % stride] = x\n",
    "            if len(self.deblocks) > 0:\n",
    "                ups.append(self.deblocks[i](x))\n",
    "            else:\n",
    "                ups.append(x)\n",
    "\n",
    "        if len(ups) > 1:\n",
    "            x = torch.cat(ups, dim=1)\n",
    "        elif len(ups) == 1:\n",
    "            x = ups[0]\n",
    "\n",
    "        if len(self.deblocks) > len(self.blocks):\n",
    "            x = self.deblocks[-1](x)\n",
    "\n",
    "        data_dict['spatial_features_2d'] = x\n",
    "        return data_dict\n",
    "\n",
    "\n",
    "\n",
    "class ConvClassificationHead(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvClassificationHead, self).__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size = 4, padding = 1, stride = 2)\n",
    "        self.act = nn.Sigmoid()  #what should it be\n",
    "    def forward(self, batch_dict):\n",
    "        breakpoint()\n",
    "\n",
    "        spatial_features_2d = batch_dict[\"spatial_features_2d\"]\n",
    "        output_prob = self.conv(spatial_features_2d)\n",
    "        if not self.training:\n",
    "            output_prob = self.act(output_prob)\n",
    "        batch_dict[\"output_prob\"] = output_prob\n",
    "        return batch_dict\n",
    "        \n",
    "    \n",
    "class SPG_CLASSIFICATION(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SPG_CLASSIFICATION, self).__init__()\n",
    "        self.vfe_module = PillarVFE(\n",
    "            model_cfg = model_cfg.VFE,\n",
    "            num_point_features=model_info_dict['num_rawpoint_features'],\n",
    "            point_cloud_range=model_info_dict['point_cloud_range'],\n",
    "            voxel_size=model_info_dict['voxel_size'],\n",
    "            grid_size=model_info_dict['grid_size'],\n",
    "            depth_downsample_factor=model_info_dict['depth_downsample_factor']\n",
    "        )\n",
    "        model_info_dict['num_point_features'] = self.vfe_module.get_output_feature_dim()\n",
    "        model_info_dict['module_list'].append(self.vfe_module)\n",
    "        \n",
    "        self.map_to_bev_module = PointPillarScatter(\n",
    "            model_cfg = model_cfg.MAP_TO_BEV,\n",
    "            grid_size = model_info_dict['grid_size']\n",
    "        )\n",
    "        model_info_dict['module_list'].append(self.map_to_bev_module)\n",
    "        model_info_dict['num_bev_features'] = self.map_to_bev_module.num_bev_features\n",
    "        \n",
    "        self.backbone_2d_module = BaseBEVBackbone(\n",
    "            model_cfg=model_cfg.BACKBONE_2D,\n",
    "            input_channels=model_info_dict['num_bev_features']\n",
    "        )\n",
    "        model_info_dict['module_list'].append(self.backbone_2d_module)\n",
    "        model_info_dict['num_bev_features'] = self.backbone_2d_module.num_bev_features\n",
    "        \n",
    "        self.classification_head = ConvClassificationHead(model_info_dict[\"num_bev_features\"],model_info_dict[\"out_classes\"])\n",
    "    def forward(self, batch_dict):\n",
    "        batch_dict = self.vfe_module(batch_dict)\n",
    "        batch_dict = self.map_to_bev_module(batch_dict)\n",
    "        batch_dict = self.backbone_2d_module(batch_dict)\n",
    "        batch_dict = self.classification_head(batch_dict)\n",
    "        return batch_dict\n",
    "\n",
    "        \n",
    "        \n",
    "model = SPG_CLASSIFICATION()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9c2e296",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-25-d1bdc77ea5a3>\u001b[0m(289)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    287 \u001b[0;31m        \u001b[0mbreakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    288 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 289 \u001b[0;31m        \u001b[0mspatial_features_2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"spatial_features_2d\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    290 \u001b[0;31m        \u001b[0moutput_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspatial_features_2d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    291 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> \n",
      "ipdb> c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['frame_id', 'gt_boxes', 'points', 'use_lead_xyz', 'voxels', 'voxel_coords', 'voxel_num_points', 'image_shape', 'batch_size', 'gt_classification', 'pillar_features', 'spatial_features', 'spatial_features_2d', 'output_prob'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dict = model(train_sample)\n",
    "out_dict.keys()\n",
    "# train_sample[\"voce\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fcc42c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 40, 496, 432])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dict[\"output_prob\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b66a0034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.return_types.min(\n",
       " values=tensor([ 0.,  0.,  0., 11.], device='cuda:0'),\n",
       " indices=tensor([  0,   0,  79, 136], device='cuda:0')),\n",
       " torch.return_types.max(\n",
       " values=tensor([  0.,   0., 262., 431.], device='cuda:0'),\n",
       " indices=tensor([   0,    0, 1549, 1268], device='cuda:0')))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5bb7b7cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6050, 32, 4])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample[\"voxels\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "03ca42fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6050, 4])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample[\"voxel_coords\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8c156402",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = pickle.load(open(\"voxelized_train_sample.p\", \"rb\"))\n",
    "load_data_to_gpu(train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "adb00531",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample[\"voxel_coords\"]\n",
    "from pcdet.ops.roiaware_pool3d.roiaware_pool3d_utils import points_in_boxes_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5374581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = train_sample[\"voxel_coords\"]\n",
    "batch = grid[grid[:,0] == 0][:,1:]\n",
    "batch = batch.unsqueeze(0)\n",
    "# batch.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275bd27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = train_sample[\"gt_boxes\"]\n",
    "# boxes = boxes[boxes[:,0] == 0][:,1:]\n",
    "# boxes.shape\n",
    "boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c10faf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mercury/anaconda3/envs/torch_env/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "kornia.losses.focal.BinaryFocalLossWithLogits"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kornia \n",
    "kornia.losses.BinaryFocalLossWithLogits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71ca5678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point_cloud_range = [0, -39.68, -3, 69.12, 39.68, 1]\n",
    "voxel_size = [0.16, 0.16, 0.2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2934f2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = []\n",
    "coords = []\n",
    "for i in range(len(voxel_size)):\n",
    "    centers.append([(x+voxel_size[i]/2) for x in np.arange(point_cloud_range[i], point_cloud_range[i+3], voxel_size[i])])\n",
    "    coords.append(np.arange(0, len(centers[-1])).tolist())\n",
    "centers = np.stack(np.meshgrid(*centers), -1).reshape(-1,3)\n",
    "coords = np.stack(np.meshgrid(*coords), -1).reshape(-1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9456ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0],\n",
       "       [  1,   0,   0],\n",
       "       [  2,   0,   0],\n",
       "       ...,\n",
       "       [ 17, 495, 431],\n",
       "       [ 18, 495, 431],\n",
       "       [ 19, 495, 431]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coords[:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e9b88ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'unsqueeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-a59a8a3307f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcenters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'unsqueeze'"
     ]
    }
   ],
   "source": [
    "centers.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8dd1acc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4285440, 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(centers, 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b2df93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
